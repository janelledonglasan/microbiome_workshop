---
title: "USF Omics Hub Microbiome Data-Analysis Workshop: hands-on exercises"
authors: "J. Oberstaller, A. Sarkar, J. Gibbons, T.E. Keller, S. Rakesh, C. Wang"
guinea-pigs: "J. Donglasan, S. Jahangiri, J. Dahrendorff"
date: "6/03/2020"
output: html_document
---
### First step will be to create a new project in RStudio, then open this markdown there. We'll include a walkthrough. ###
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
# setting the source will not be necessary if all packages are pre-loaded into a Docker container
#source("Rsource/Hub_microbiome_workshop_source.R")
```
# USF Omics Hub Microbiome Workshop Day 2: Generating ASV tables from microbiome-sample sequencing data #

# Pipeline-overview #

*Goal*: The purpose of this analysis is to obtain an Amplicon Sequence Variant (ASV) table for all of our microbiome-sample example-data.

*Input data*: We will start with demultiplexed fastq files for all samples. *This analysis is for paired-end data.* Thus, for each sample, there will be two files, named according to Illumina platform conventions:

  1. Forward-reads, named *_R1_001.fastq
  2. Reverse-reads, named *_R2_001.fastq

## Before we begin ##
Before we begin, let's take a moment to get organized. The importance of documentation and good record-keeping are *essential* to producing high-quality and reproducible computational analyses, just as they are at the bench! 

We recommend you keep your analyses organized by project (just as we organized this example). Looking around: 
    
  - **Rdata**: this folder contains our input .fastq.gz files and our input database of 16S-sequences that we'll use to identify taxa present in our samples.

  - **Ranalysis**: this folder contains any scripts we create to analyze our data, like this R-Markdown (.Rmd) document.
  - **Routput**: we will direct any output data-files from our analyses to this folder.
  - **Rfigs**: we will direct any figures we generate from our analyses to this folder.
  - **Rsource**: this folder contains any R source-scripts we create to set up our environment for our analyses--custom functions, which packages to load, etc. etc. You don't need to worry about this one since we made it for you.
  
      You can think of these as set-up scripts--just load it at the beginning of your session and forget about it.
  

## Begin analyses ##

Now back to RStudio.

```{r load}
# Let's load the appropriate R packages (dada2 and a few others) for the analysis. All packages should have automatically been installed when you ran this markdown.
library(dada2)
library(GUniFrac)
library(simEd)
library(tictoc)
```

*Important:* make sure to note the package-version of each package you're using!
```{r version-check}
packageVersion("dada2")
packageVersion("GUniFrac")
packageVersion("simEd")
packageVersion("tictoc")
```

Next we will load our input-data into R:
```{r}
# Let's give the path to the directory where all the fastq files are stored a name ("demo_microbiome_fasqfiles").
demo_microbiome_fasqfiles <- "Rdata/fastq"
```

```{r separate_fastq_files}
# Check that the fastq directory contains all our fastq files
list.files(demo_microbiome_fasqfiles)
# Now make two variables to separate and store the forward and the reverse reads
demo_F <- sort(list.files(demo_microbiome_fasqfiles,
                          pattern="_R1_001.fastq",
                          full.names = TRUE))
demo_R <- sort(list.files(demo_microbiome_fasqfiles,
                          pattern="_R2_001.fastq",
                          full.names = TRUE))
# Extract filenames of all samples for future steps in the analysis
demo_samplenames <- sapply(strsplit(basename(demo_F),
                                    "_"),
                           '[', 1)
```

## JO NOTE: We'll be moving at a fast pace and you'll be encountering lots of new functions. Remember you can type ?function_name() into the console at any time to get an explanation for what any function does and available options/parameters. The information will be dense, but helpful!


## Evaluating data-quality ##

Let's check our data-quality by making plots and viewing them directly in RStudio. Your plots will appear in the RStudio "Plots" pane to the lower-right.

```{r check_fastq_quality}
# First we'll check the forward-reads:
plotQualityProfile(demo_F[1:4],
                   n=1e+06)
# Now let's plot to check the data-quality of our reverse-reads.
plotQualityProfile(demo_R[1:4],
                   n=1e+06)
```

Let's also output the plots as .pdf files so we can view them later. They'll be saved in the Rfigs directory.
  *Helpful tip: It is important to save any data or figures you generate in R that you want to keep to file; they are not saved when you quit RStudio, and you'll have to regenerate them!*
  
```{r make_quality_pdfs, include=TRUE, echo=FALSE, warning=FALSE}
## save data-quality plot for forward-reads:
pdf("Rfigs/demo_F_quality.pdf",
    width = 12,
    height = 8,
    pointsize = 8)
plotQualityProfile(demo_F[1:4],
                   n=1e+06)
dev.off()
# save the data-quality plot for our reverse-reads:
pdf("Rfigs/demo_R_quality.pdf",
    width = 12,
    height = 8,
    pointsize = 8)
plotQualityProfile(demo_R[1:4],
                   n=1e+06)
dev.off()
```

## Filter reads based on data-quality ##

The next step is to filter the sequences appropriately, the parameters for which will depend on the data. 

Conceptually, we will discard the bad reads, trim the ends of the good reads, and then save the trimmed good reads to a new directory.

First we will specify the path and name the output-files to which the good sequences will be written. 
  **the directory and output-files we specify here will be created in the next step (filterAndTrim).**
```{r filter_bad_reads1}
# The first step here is to specify the path and name the output-files to which the good sequences will be written. 
  # the directory and output-files we specify here will be created in the next step (filterAndTrim).

## JO NOTE: remove references to the "here" package; instead begin tutorial with how to set up a project in RStudio
demo_goodF <- file.path("Routput/demo_good_filtered",
                        paste0(demo_samplenames,
                               "F_good.fastq.gz"))
demo_goodR <- file.path("Routput/demo_good_filtered",
                        paste0(demo_samplenames,
                               "R_good.fastq.gz"))
names(demo_goodF) <- demo_samplenames
names(demo_goodR) <- demo_samplenames
```


Now we perform the very important step of filtering and trimming each fastq. 

*These parameters are flexible and should depend on your data!*

```{r filter_bad_reads2}
demo_good_proper <- filterAndTrim(demo_F,
                                  demo_goodF,
                                  demo_R,
                                  demo_goodR,
                                  trimLeft = c(17, 21),
                                  truncLen = c(145, 135),
                                  maxN = 0,
                                  truncQ = 2,
                                  minQ=1,
                                  maxEE = c(2, 4),
                                  rm.phix = TRUE,
                                  n = 1e+5,
                                  compress = TRUE,
                                  verbose = TRUE)
# save the output of previous step (a summary table indicating how many reads there were for each sample before and after quality-filtering):
write.table(demo_good_proper,
            file = "Routput/demo_filteredout.txt",
            sep = "\t",
            quote = FALSE)
```




## Dereplicate sequences in each sample ##

Dereplicating the data collapses together reads that encode the same sequence this ends up saving computational time in later stages. (see section 4 https://bioconductor.org/packages/devel/bioc/vignettes/dada2/inst/doc/dada2-intro.html)
```{r dereplicate}
# The next step is to dereplicate the sequences in each sample
derep_demo_F <- derepFastq(demo_goodF,
                           n = 1e+06,
                           verbose = TRUE)
derep_demo_R <- derepFastq(demo_goodR,
                           n = 1e+06,
                           verbose = TRUE)
names(derep_demo_F) <- demo_samplenames
names(derep_demo_R) <- demo_samplenames
```


## Calculate and plot error-rates ##

Now let's calculate the error-rates (see below) for the forward and reverse sequences, plot them directly in RStudio and save to .pdf.

[see section 5 ]
(https://bioconductor.org/packages/devel/bioc/vignettes/dada2/inst/doc/dada2-intro.html)
From the dada2 vignette:

" The dada algorithm uses a parametric model of the errors introduced by PCR amplification and sequencing. Those error parameters typically vary between sequencing runs and PCR protocols, so our method provides a way to estimate those parameters from the data itself."

```{r set_seed}
# In order to make the results reproducible when carried out multiple times, we use the set.seed function.

set.seed(56456)
```

```{r error_calc}
## forward-reads:
demo_error_F <- learnErrors(derep_demo_F,
                            nbases = 1e+07,
                            randomize = TRUE,
                            MAX_CONSIST = 12,
                            multithread = TRUE,
                            verbose = TRUE)
plotErrors(demo_error_F,
           obs = TRUE,
           nominalQ = TRUE)
## and reverse-reads:
demo_error_R <- learnErrors(derep_demo_R,
                            nbases = 1e+07,
                            randomize = TRUE,
                            MAX_CONSIST = 12,
                            multithread = TRUE,
                            verbose = TRUE)
plotErrors(demo_error_R, obs = TRUE,
           nominalQ = TRUE)
```

We'll also save both plots to .pdf in our Rfigs directory for our records:
```{r error_plots, include=TRUE, echo=FALSE, warning=FALSE}
pdf("Rfigs/demo_error_F_plot.pdf",
    width = 10,
    height = 10,
    pointsize = 8)
plotErrors(demo_error_F,
           obs = TRUE,
           nominalQ = TRUE)
dev.off()
pdf("Rfigs/demo_error_R_plot.pdf",
    width = 10,
    height = 10,
    pointsize = 8)
plotErrors(demo_error_R,
           obs = TRUE,
           nominalQ = TRUE)
dev.off()
```


## Running dada2: Calculating ASVs ##

Now it is time to run the actual dada2 algorithm to determine the ASVs in the dataset.

  ** This step is run separately for forward and reverse sets of paired-end reads **

```{r running_dada2}
demo_dada_F <- dada(derep_demo_F,
                    err=demo_error_F,
                    pool = TRUE,
                    multithread = TRUE)
demo_dada_R <- dada(derep_demo_R,
                    err=demo_error_R,
                    pool = TRUE,
                    multithread = TRUE)
# Let's see how many sequence variants we have got in the forward set
demo_dada_F[[1]]
```


Next we will merge the forward and reverse sets (the paired-end reads for all our samples), and output a sequence-table of all ASVs.

```{r mergePairs}
demo_merged <- mergePairs(demo_dada_F,
                          derep_demo_F,
                          demo_dada_R,
                          derep_demo_R,
                          minOverlap = 20,
                          maxMismatch = 0,
                          verbose = TRUE)
# Let's make a sequence table of all the ASVs
demo_sequence_table <- makeSequenceTable(demo_merged,
                                         orderBy = "abundance")
# We can check the distribution of the ASVs by length
table(nchar(getSequences(demo_sequence_table)))
```

## Filtering chimeric sequences ##

```{r}
# Remove chimeric sequences
demo_nochim <- removeBimeraDenovo(demo_sequence_table,
                                  method = "consensus",
                                  minFoldParentOverAbundance = 1,
                                  verbose = TRUE,
                                  multithread = TRUE)
# Let's see how many ASVs remain after filtering chimeric sequences:
dim(demo_nochim)
# Let's see the proportion of sequences we retained after filtering for chimeric sequences:
sum(demo_nochim)/sum(demo_sequence_table)
```

Now we have completed all the filtering, trimming, cleanup etc. to arrive at our final data-set. Here we should check and record how many sequences we retained after each step.
  
  ** This test is important for trouble-shooting purposes. **

```{r}  
# Create a function to calculate reads retained
fetch_numbers <- function(a) sum(getUniques(a))
# Then apply this function to the output of each step in our pipeline to generate a counts-table of reads remaining after each step
demo_track_steps <- cbind(demo_good_proper,
                          sapply(demo_dada_F,
                                 fetch_numbers),
                          sapply(demo_dada_R,
                                 fetch_numbers),
                          sapply(demo_merged,
                                 fetch_numbers),
                          rowSums(demo_nochim))
colnames(demo_track_steps) <- c("input",
                                "filtered",
                                "denoisedF",
                                "denoisedR",
                                "merged",
                                "nochim")
rownames(demo_track_steps) <- demo_samplenames
# And save the output to a new file for our records:
write.table(demo_track_steps,
            file = "Routput/demo_filtering_steps_track.txt",
            sep = "\t",
            quote = FALSE)
```

## Assign taxonomy to all ASVs ##

We will next determine taxa present in our samples using the Silva database v.132.


dada2 helpfully maintains specially formatted databases for 3 of the most popular 16S microbiome-databases: Silva, Greengenes, and RDP (also UNITE for ITS).

We will be using the [dada2 Silva database](https://zenodo.org/record/1172783#.XcClW9VOnb1)

  *You downloaded this file to your Rdata directory previously (Rdata/Silva_db/silva_nr_v132_train_set.fa)*


```{r taxonomy}
demo_taxonomy <- assignTaxonomy(demo_nochim,
                                "Rdata/Silva_db/silva_nr_v132_train_set.fa",
                                minBoot = 80,
                                verbose = TRUE,
                                multithread = TRUE)
write.table(demo_taxonomy,file = "Routput/demo_taxaout.txt",
            sep = "\t",
            quote = FALSE)
```

Now we will generate output-files critical for further analyses and data-visualization. These include:

    a table summarizing ASVs by taxa
    a fasta-file of all ASVs
    a table of ASV-counts per sample (the OTU-table)

```{r make_final_tables}    
# Let's create a table by replacing the ASV sequences with ids (ASV_1, ASV_2 etc.) and their corresponding classifications
demo_taxa_summary <- demo_taxonomy
row.names(demo_taxa_summary) <- NULL
head(demo_taxa_summary)
# Let's make a file listing all the ASVs and their sequences in fasta format
demo_asv_seqs <- colnames(demo_nochim)
demo_asv_headers <- vector(dim(demo_nochim)[2],
                           mode = "character")
for (i in 1:dim(demo_nochim)[2]) {demo_asv_headers[i] <- paste(">ASV",
                                                               i,
                                                               sep = "_")}
demo_asv.fasta <- c(rbind(demo_asv_headers,
                          demo_asv_seqs))
write(demo_asv.fasta,
      file = "Routput/demo_out_asv.fasta")
# At this step, we need to make a table of ASV counts for each sample (which is going to be most important for all statistical analyses)
demo_asv_tab <- t(demo_nochim)
row.names(demo_asv_tab) <- sub(">",
                               "",
                               demo_asv_headers)
write.table(demo_asv_tab,
            file = "Routput/demo_asv_counts.tsv",
            sep = "\t",
            quote=FALSE,
            col.names = NA)
# Finally, let's make a table with the taxonomy of all the ASVs
demo_asv_taxa <- demo_taxonomy
row.names(demo_asv_taxa) <- sub(">",
                                "",
                                demo_asv_headers)
write.table(demo_asv_taxa,file = "Routput/demo_asvs_taxonomy.tsv",
            sep = "\t",
            quote=FALSE,
            col.names = NA)
dim(demo_asv_taxa)
# We'll also need to make fake sample-bmi data for tomorrow's visualization-exercises (phyloseq)
bmi <- c('obese',
         'obese',
         'lean',
         'lean')
demo_fake_sample_data <- data.frame(bmi_group=bmi)
rownames(demo_fake_sample_data) <- c("demo1",
                                     "demo2",
                                     "demo3",
                                     "demo4")
write.table(demo_fake_sample_data,
            file = "Routput/made_up_sample_data.tsv",
            sep="\t",
            quote=FALSE)
```


Tomorrow, we'll plot the data we analyzed today.


## ** End Day 2 ** ##

**USF Omics Hub Microbiome Workshop Day 3: Visualizing microbiome-data**

# Phyloseq object creation from dada2 data

We'll first be plotting the example-data we analyzed yesterday. We've started a new project for day3 of the workshop, and you'll notice the directory structure is the same as the project we set up yesterday. Your "Rdata"" folder contains the output-data from yesterday we need for plotting.

First we'll read in and format our data for phyloseq.

```{r phyloseq-create}
infile_asv_counts<-here::here("Rdata/demo_asv_counts.tsv")
infile_asv_tax<-here::here("Rdata/demo_asvs_taxonomy.tsv")
infile_sample_data<-here::here("Rdata/made_up_sample_data.tsv")

df_asv_counts<-read.delim(infile_asv_counts)
df_asv_tax<-read.delim(infile_asv_tax)
df_sample_data<-read.delim(infile_sample_data)
#rownames(df_sample_data)<-df_sample_data[,1]
#Convert the data to matrix format
m_asv_counts<-as.matrix(df_asv_counts[2:length(df_asv_counts)])
rownames(m_asv_counts)<-df_asv_counts[,1]

m_asv_tax<-as.matrix(df_asv_tax[2:length(df_asv_tax)])
rownames(m_asv_tax)<-df_asv_tax[,1]

##There are 3 possible components to a phyloseq object: otu_table, sample_data, tax_table

pseq<-phyloseq(otu_table(m_asv_counts,taxa_are_rows = T),tax_table(m_asv_tax),sample_data(df_sample_data))



```

# Inspecting the phyloseq object

```{r phyloseq-inspect}

##Lets look at the data in our phyloseq object
##The functions used to create the components of a phyloseq object
##can be used to view the components of a phyloseq object
View(otu_table(pseq))
View(sample_data(pseq))
View(tax_table(pseq))

```


# Get yer alpha and beta diversities here

Step one/figure one in many/most microbiome-analyses is some description of the alpha/beta diversity. What are alpha and beta diversity? *alpha diversity* asks the questions

    1) How many taxa/species, and 
    2) how different (are taxa/species equally abundant?)

*Beta diversity* asks whether composition varies across environment.

```{r diversity}
####Diversity####
##Diversity measures are calculated using the function alpha
tab<-alpha(pseq,index="all")
kable(head(tab))
##Return observed richness with given detection thresholds
tab<-richness(pseq)
kable(head(tab))

##Returns measures of how unequal the taxa distrubtions are
tab<-dominance(pseq,index="all")
kable(head(tab))

##Returns the most abundant taxa in each sample
dominant(pseq)
##Can specify the taxonimic level you want to know the dominance for
##Default is the OTU or ASV
dominant(pseq,level="Phylum")

##Evenness: Measures of how even the "species" are within a sample
tab<-evenness(pseq,index="all")
kable(head(tab))

####Alpha diversity plots####
##Only retain taxa that are present and calculate diversity indices
ps1<-prune_taxa(taxa_sums(pseq)>0,pseq)
tab<-diversity(ps1,index="all") #fix deprecation warning
View(tab)
##Get the sample meta data
ps1.meta<-meta(ps1)
kable(head(ps1.meta))
##Add the diversity table to the metadata
ps1.meta$Shannon<-tab$shannon
ps1.meta$InverseSimpson<-tab$inverse_simpson
##We have created a table that we can use for plotting data
View(ps1.meta)
```




# bmi plots

```{r bmi}
##Compare the differences in Shannon index between BMI groups
##Create a list of pairwise comparisions
bmi<-levels(ps1.meta$bmi_group) #get the variables
bmi.pairs<-combn(bmi,2,simplify = F)
print(head(ps1.meta))
##Create a violin plot
p1<-ggviolin(ps1.meta,x="bmi_group",y="Shannon",add="boxplot",fill="bmi_group",
             palette = c("#a6cee3", "#b2df8a", "#fdbf6f"))
print(p1)

dev.off()
```


We'll also save the plot to .pdf in our Rfigs directory for our records:
```{r error_plots, include=TRUE, echo=FALSE, warning=FALSE}

pdf(here::here("Rfigs/bmi.plot.pdf"), width = 10, height = 10, pointsize = 8)
ggviolin(ps1.meta,x="bmi_group",y="Shannon",add="boxplot",fill="bmi_group",
             palette = c("#a6cee3", "#b2df8a", "#fdbf6f"))
dev.off()


```


# beta diversity plots on larger example dataset

Now that we've gone through most of the analysis-process with our small data-set--let's switch to using a real dataset so we can make more statistically valid comparisons. All the processing to generate the data-tables for visualization has already been done.

```{r dietswap}


####Beta diversity and microbiome divergence####

data(dietswap)
pseq<-dietswap
##Lets look at the new data set we will be using
View(otu_table(pseq))
View(tax_table(pseq))
View(sample_data(pseq))
##Calculate group divergences within the bmi groups
##This measure is sensitive to sample size
##If using NGS data it is recommended to subsample or bootstrap to avoid
##bias. This sample data is from a microarray so we don't have to 
##worry about uneven sample sizes
b.obese<-divergence(subset_samples(pseq,bmi_group="obese"),method="bray")
b.lean<-divergence(subset_samples(pseq,bmi_group="lean"),method="bray")
boxplot(list(Obese=b.obese,Lean=b.lean))
##The amount of diversity between samples in each group appears equal
```

# alpha diversity in dietswap

```{r alpha-dietswap}
##Lets Check the alpha diversity on this set
##Only retain taxa that are present and calculate diversity indices
ps1<-prune_taxa(taxa_sums(pseq)>0,pseq)
tab<-diversity(ps1,index="all") #update diversity function
View(tab)
##Get the sample meta data
ps1.meta<-meta(ps1)
kable(head(ps1.meta))
##Add the diversity table to the metadata
ps1.meta$Shannon<-tab$shannon
ps1.meta$InverseSimpson<-tab$inverse_simpson
##We have created a table that we can use for plotting data
View(ps1.meta)

```


# Shannon Diversity across groups

```{r shannon-dietswap}
##Compare the differences in Shannon index between BMI groups
##Create a list of pairwise comparisions
bmi<-levels(ps1.meta$bmi_group) #get the variables
bmi.pairs<-combn(bmi,2,simplify = F)
##Create a violin plot
p1<-ggviolin(ps1.meta,x="bmi_group",y="Shannon",add="boxplot",fill="bmi_group",
             palette = c("#a6cee3", "#b2df8a", "#fdbf6f"))
print(p1)

##Add stats (Wilcoxon test)--From the documentation. I think the warnings are because there are not enough replicates
p1<-p1+stat_compare_means(comparisons = bmi.pairs,method="wilcox.test")
print(p1)

##Diversity seems to be a little higher in the overweight group an not
##significantly different between the lean and obese groups
##This is different from what is usually reported.
## Can anyone think of an explanation? 


```

# beta diversity over time helper functions

Write some functions here to calculate beta diversity directly, also dropping the "ED" group because it has some problems.

```{r beta-funcs}

##Let's now quantify how a persons microbiome changes over time
##Quantify beta diversity within subjects over time
subject_beta_diversity_over_time<-function(phyloseq_obj){
  betas<-list()
  df_meta<-meta(phyloseq_obj)
  ##We're dropping the ED group because the way it is encoded
  ##makes it difficult to interpret 
  df_meta<-df_meta[which(df_meta$group != "ED"),]
  View(df_meta)
  groups<-as.character(unique(df_meta$group))
  for(g in groups){
    df<-subset(meta(pseq),group==g)
    beta<-c()
    
    for(subj in df$subject){
      dfs<-subset(df,subject==subj)
      #Check that subject has 2 time points
      if(nrow(dfs)==2){
        s<-as.character(dfs$sample)
        #Calculate the beta diversity directly
        beta[[subj]]<-1-cor(abundances(phyloseq_obj)[,s[[1]]],
                            abundances(phyloseq_obj)[,s[[2]]],
                            method="spearman")
      }
    }
    betas[[g]]<-beta
  }
  return(betas)
}


betas<-subject_beta_diversity_over_time(pseq)
##DI is during diet intervention HE is baseline
##The end of dietary intervention
##Subjects microbiomes became more similar when they were eating 
##the experimental diets
boxplot(betas)

##Calculate change in beta diversity (community dismilarity) over time within a single individual
##Identify the subject with the longest time series (most time points)
s<-names(which.max(sapply(split(meta(pseq)$timepoint,meta(pseq)$subject),function (x){length(unique(x))})))
##Pick the metadata for this subject and sort the samples by time
library(dplyr)
df<-meta(pseq) %>% filter(subject==s) %>% arrange(timepoint)

calculate_individual_beta_diversity_over_time<-function(df,pseq){
  ##Calculate how the sample diversity changes relative to baseline
  beta<-c(0,0) #Baseline similarity
  #s0<-subset(df,time=0)$sample
  s0<-df[which(df$timepoint==1),]$sample
  #print(s0)
  for(tp in df$timepoint[-1]){
    #Pick the samples for this subject
    #If the same time point has more than one sample, pick one at random
    st<-sample(subset(df,timepoint==tp)$sample,1)
    
    a<-abundances(pseq)
    #print(a[,s0])
    b<-1-cor(a[,s0],a[,st],method="spearman")
    beta<-rbind(beta,c(tp,b))
  }
  colnames(beta)<-c("time","beta")
  beta<-as.data.frame(beta)
  return(beta)
}

df_beta<-calculate_individual_beta_diversity_over_time(df,pseq)

library(ggplot2)
p<-ggplot(df_beta,aes(x=time,y=beta))+geom_point()+geom_line()
print(p)


```

# Microbiome Composition

What are the phylum composing the core microbiome?

```{r composition}
####Microbiome Composition plots####

##Compute relative level of each taxa
pseq.rel<-microbiome::transform(pseq,"compositional")
##Return a phyloseq object of the core microbiota

pseq.core<-core(pseq.rel,detection = 0,prevalence = 0.5)

pseq.core<-subset_samples(pseq.core,group="HE" & timepoint.within.group==1)
pseq.core.phylum<-aggregate_taxa(pseq.core,level="Phylum")


theme_set(theme_bw(21))

##Remember to save as 10x10
p <- plot_composition(pseq.core.phylum,group_by="nationality") +
  guides(fill = guide_legend(ncol = 1)) +
  labs(x = "Samples", y = "Relative abundance",
       title = "Relative abundance data",
       subtitle = "Subtitle",
       caption = "Caption text.")+
  scale_fill_manual(values = default_colors("Phylum"))


print(p)
```


# More detailed analysis & plotting of the core microbiome

Core microbiome is calculated with a combination of detection (otu percentage), and prevalence (sample percentence)
 cutoffs).
 
```{r core}
 
 ####Core microbiome####
##Return the names of the taxa that excede given prevelence and detection thresholds
core.taxa.standard<-core_members(pseq.rel,detection=0,prevalence = 50/100)
##Return a phyloseq object of the core microbiota
pseq.core<-core(pseq.rel,detection = 0,prevalence = 0.5)
##Get taxa names
core.taxa<-taxa(pseq.core)
##Sum of abundances of the core members in each sample (Fraction of each sample composed of the
##core groups)
core.abundance<-sample_sums(core(pseq.rel,detection = 0.01,prevalence = 0.95))
##Core heatmaps
##Core with compositionals
prevalences <- seq(.05, 1, .05)
detections <- 10^seq(log10(1e-3), log10(.2), length = 10)

# Also define gray color palette
gray <- gray(seq(0,1,length=5))
p <- plot_core(pseq.rel, plot.type = "heatmap", colours = gray,
               prevalences = prevalences, detections = detections) +
  xlab("Detection Threshold (Relative Abundance (%))")+
  theme(axis.text=element_text(size=5),legend.text=element_text(size=10))
print(p)

##Check abundance of specific taxon
plot_density(pseq.rel,variable = "Dialister",log10=TRUE)+xlab("log10 Relative Abundance")

```

# ordination - representing data in 2d space, usually followed by some clustering

Ordination is a general ecological framework to recast the high-dimensional OTU data in a 2d plane, usually followed by some clustering. If things go well, samples will cluster relative to some sample/treatment data (eg bmi, or as seen below, nationality)

There are a variety of ordination methods and distances, and all will affect the grouping slightly

For a more extensive example on the differences between examples, [see](https://joey711.github.io/phyloseq/plot_ordination-examples.html), which is the underlying function called by plot_landscape().

```{r ordination: PCoA}
####Ordination plotting####
p<-plot_landscape(pseq.core,method="PCoA",distance="bray",col="nationality")+
  labs(title="PCoA/Bray-Curtis")
print(p)
```
```{r ordination: NMDS}
####Ordination plotting####
set.seed(423542)
p<-plot_landscape(pseq.core,method="NMDS",distance="bray",col="nationality")+
  labs(title="NMDS/Bray-Curtis")
print(p)
```

```{r DESeq2}
library(DESeq2)
##Make AFR (African) the reference group
sample_data(pseq)$nationality<-relevel(sample_data(pseq)$nationality, "AFR")

##Subset the data so that you only have the first baseline measurements
pseq.subset<-subset_samples(pseq,group=="HE" & timepoint.within.group==1)
##Convert data into a format that can be used by DESeq
##Group samples by nationality
pseq.deseq<-phyloseq_to_deseq2(pseq.subset, ~nationality)
##Perform normalization and differential abundance tests
pseq.deseq<-DESeq(pseq.deseq,test="Wald",fitType="parametric")
##Get and filter the results
res<-results(pseq.deseq,cooksCutoff = FALSE)
alpha<-0.01
res.sig<-res[which(res$padj<=alpha),]
res.sig <-cbind(as(res.sig, "data.frame"), as(tax_table(pseq)[rownames(res.sig), ], "matrix"))
##Write results to file
write.csv(res.sig,file=here::here("Routput/differential_taxa_abundance.csv"))
```